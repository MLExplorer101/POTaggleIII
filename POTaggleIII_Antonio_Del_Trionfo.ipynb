{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_EzfFy4RGwR"
   },
   "source": [
    "# **Po Taggle III - May 2024**\n",
    "\n",
    "# What is the problem to solve\n",
    "In order to summarize an earnings call transcript, we want to extract all the partecipants, understand for each of them what are the relevant sentences from which we can extract financial information, that then will be listed in a financial table and in a bullets points list, differenciating sentences from the PRESENTATION and Q&A session.\n",
    "# How it is been solved and implemented it\n",
    "In order to extract relevant financial sentences from the transcript, i have implemented a NER(Named Entity Recognition) pre-trained model(distillied BERT), fine-tuned with custom entities: ***Reported Value***, ***Multiplier***, ***Currency***\tand ***Granular Concept***.\n",
    "\n",
    "In order to train and fine tune it i have used Spacy library. \n",
    "I have used this(<a href=\"train_model.ipynb\" target=\"_blank\">train_model.ipynb</a>) other notebook to train the model with the custom entities.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QmAE-KTN8Os"
   },
   "source": [
    "# Install and import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "s_zHhBEZKbhu"
   },
   "outputs": [],
   "source": [
    "# install any necessary packages\n",
    "#python -m pip install fitz\n",
    "#python -m pip install PyMuPDF\n",
    "#python -m pip install -U spacy\n",
    "#python -m pip install spacy_transformers\n",
    "#python -m pip install python-docx\n",
    "\n",
    "import spacy\n",
    "import spacy_transformers\n",
    "import docx\n",
    "import sys\n",
    "import fitz\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKPTePg_OLen"
   },
   "source": [
    "# Load the trained model\n",
    "**NOTE:** in order to fine tune it i have only used the financial info extracted from Novartis, Carlesberg and .... summarizations file provided, annotating them manully using https://tecoholic.github.io/ner-annotator/ . </br>\n",
    "The ***model score is 0.62*** due to limited annotated data, time and resources, but its result seems to be already good, but definetly can be improved with more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32280,
     "status": "ok",
     "timestamp": 1715978933603,
     "user": {
      "displayName": "antonio del trionfo",
      "userId": "08129683538845839311"
     },
     "user_tz": -60
    },
    "id": "_G-XLdAd_E2n",
    "outputId": "cf74b920-6ab3-4f0d-c1f2-56bc8f12dfd9"
   },
   "outputs": [],
   "source": [
    "# Load the trained spaCy NER model from the specified path\n",
    "nlp = spacy.load('trained_models/output/model-last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeQhXXW4Qx5h"
   },
   "source": [
    "# Infer the model if you want to test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1715891315953,
     "user": {
      "displayName": "antonio del trionfo",
      "userId": "08129683538845839311"
     },
     "user_tz": -60
    },
    "id": "OxuiIaFxOoCx",
    "outputId": "47c5e407-eb60-4788-a7d9-6118d5514a1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARR   ->>>>   GRANULAR CONCEPT\n",
      "$   ->>>>   CURRENCY\n",
      "432   ->>>>   REPORTED VALUE\n",
      "million   ->>>>   MULTIPLIER\n"
     ]
    }
   ],
   "source": [
    "# Process the extracted text using the loaded spaCy NER model\n",
    "doc = nlp(\"Net new Digital Media ARR $432 million\")\n",
    "\n",
    "# Iterate through the named entities (entities) recognized by the model\n",
    "for ent in doc.ents:\n",
    "  # Print the recognized text and its corresponding label\n",
    "  print(ent.text, \"  ->>>>  \", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Paragraphs, Conference and Corporate partecipatns from the transcript given in input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the PDF file\n",
    "fname = 'test_dataset/Adobe.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-QJV9nxfN5S3"
   },
   "outputs": [],
   "source": [
    "def extract_partecipants(partecipantsList):\n",
    "  conf_participants = []\n",
    "  corp_participants = []\n",
    "  idx=-1\n",
    "\n",
    "  conf = False\n",
    "  corp = False\n",
    "  for row in partecipantsList[6:]:\n",
    "    idx += 1\n",
    "\n",
    "    if row == \"CORPORATE PARTICIPANTS\":\n",
    "      conf = False\n",
    "      corp = True\n",
    "      continue\n",
    "\n",
    "    if row == \"CONFERENCE CALL PARTICIPANTS\" :\n",
    "            conf = True\n",
    "            corp = False\n",
    "            continue\n",
    "\n",
    "    if row == \"PRESENTATION\":\n",
    "      return corp_participants , conf_participants ,idx\n",
    "\n",
    "    if row == \" \":\n",
    "      continue\n",
    "\n",
    "    if corp:\n",
    "      corp_participants.append(row)\n",
    "\n",
    "    if conf:\n",
    "      conf_participants.append(row)\n",
    "\n",
    "def extract_parag(txt):\n",
    "  newParag = True\n",
    "  speaker = 'Unknown'\n",
    "  parag = ''\n",
    "  d = dict()\n",
    "  arr = []\n",
    "  txt_len = len(txt)\n",
    "  idx = 0\n",
    "  for line in txt:\n",
    "    idx +=1\n",
    "\n",
    "    if line == ' ':\n",
    "      newParag = True\n",
    "      arr.append({'speaker':speaker.strip(), 'paragraph':[parag]})\n",
    "      speaker = 'Unknown'\n",
    "      parag = ''\n",
    "      continue\n",
    "\n",
    "    if newParag:\n",
    "      speaker = line\n",
    "      newParag = False\n",
    "    else:\n",
    "      parag = parag +\" \"+ line\n",
    "      if idx == txt_len:\n",
    "        arr.append({'speaker':speaker.strip(), 'paragraph':[parag]})\n",
    "\n",
    "  return arr\n",
    "\n",
    "# Open the PDF document using PyMuPDF (fitz)\n",
    "doc = fitz.open(fname)\n",
    "\n",
    "# Initialize an empty string to store the extracted text from the PDF\n",
    "text = \" \"\n",
    "\n",
    "# Display the extracted text\n",
    "title=''\n",
    "\n",
    "for page in doc:\n",
    "  spl = str(page.get_text()).splitlines()\n",
    "\n",
    "  #Extract Title\n",
    "  if page.number == 0 :\n",
    "    title=spl[7]\n",
    "    continue\n",
    "\n",
    "  #Extract Conference and Corporate\n",
    "  if page.number == 1 :\n",
    "    corp_participants , conf_participants , end_index = extract_partecipants(spl)\n",
    "    text = text + '\\n'.join(spl[end_index+8:]) + '\\n'\n",
    "    continue\n",
    "\n",
    "  #On the last page skip the diclaimer\n",
    "  if page.number == len(doc)-1:\n",
    "    text = text + '\\n'.join(spl[6:len(spl)-11]) + '\\n'\n",
    "    continue\n",
    "\n",
    "  text = text + '\\n'.join(spl[6:]) + '\\n'\n",
    "\n",
    "parags = extract_parag(text.splitlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for infering the sentences and create the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "Mo4w_XCyM7nt"
   },
   "outputs": [],
   "source": [
    "def add_title(doc):\n",
    "  # Add a paragraph to the document\n",
    "  p = doc.add_paragraph()\n",
    "\n",
    "  # Add some formatting to the paragraph\n",
    "  p.paragraph_format.line_spacing = 1\n",
    "  p.paragraph_format.space_after = 0\n",
    "\n",
    "  # Add more text to the same paragraph\n",
    "  run = p.add_run(\"Summarization \" + title)\n",
    "\n",
    "  # Format the run\n",
    "  run.bold = True\n",
    "  run.font.name = 'Arial'\n",
    "  run.font.size = docx.shared.Pt(12)\n",
    "\n",
    "def add_corp_partecipants(doc):\n",
    "  add_heading(\"CORPORATE PARTICIPANTS\")\n",
    "\n",
    "  p2 = doc.add_paragraph()\n",
    "  p2.paragraph_format.line_spacing = 2\n",
    "  run2 = p2.add_run(\"\")\n",
    "  run2.font.name = 'Arial'\n",
    "  run2.font.size = docx.shared.Pt(8)\n",
    "\n",
    "  for part in corp_participants:\n",
    "    run2.add_break()\n",
    "    run2.add_text(part)\n",
    "\n",
    "def add_conf_partecipants(doc):\n",
    "  add_heading(\"CONFERENCE CALL PARTICIPANTS\")\n",
    "\n",
    "  p2 = doc.add_paragraph()\n",
    "  p2.paragraph_format.line_spacing = 2\n",
    "  run2 = p2.add_run(\"\")\n",
    "  run2.font.name = 'Arial'\n",
    "  run2.font.size = docx.shared.Pt(8)\n",
    "\n",
    "  for part in conf_participants:\n",
    "    run2.add_break()\n",
    "    run2.add_text(part)\n",
    "\n",
    "def clean_data_from_infer_errors(list_data):\n",
    "  list_presentation_cleaned = list()\n",
    "  #remove item with no numeric or not reported, this might mean an error in the inference\n",
    "  for sentence in list_data:\n",
    "    rep_val = False\n",
    "    labels = sentence['entity']\n",
    "    for ent in labels.ents:\n",
    "      tx = ent.text.replace(',','').replace('.','')\n",
    "      #print(ent.text, \"........\", ent.label_ , \"-----\", tx.isdigit())\n",
    "      if ent.label_ == \"REPORTED VALUE\" and tx.isdigit() :\n",
    "        rep_val = True\n",
    "\n",
    "    if rep_val:\n",
    "        list_presentation_cleaned.append(sentence)\n",
    "\n",
    "  return list_presentation_cleaned\n",
    "\n",
    "def detect_umbrella_concept(granular_concept):\n",
    "    \n",
    "  umbrella_concept_dict = {'sales':['sales','arr','revenue'],\n",
    "                           'cash & cash equivalents':['cash'],\n",
    "                           'margin':['margin'],\n",
    "                           'cash flow':['free cash flow','cash flow from operations'],\n",
    "                            'debt':['debt'],\n",
    "                            'dps':['dividend'],\n",
    "                            'eps':['reported earnings per share'],\n",
    "                            'profit':['net profit','operating income']\n",
    "                          }  \n",
    "  \n",
    "  for key in umbrella_concept_dict.keys():\n",
    "      if any(ext in granular_concept.lower() for ext in umbrella_concept_dict.get(key)):\n",
    "          return str(key)\n",
    "          break\n",
    "\n",
    "  return \"\"\n",
    "\n",
    "\n",
    "def add_fin_table(doc,list_presentation):\n",
    "  add_heading(\"FINANCIAL TABLE\")\n",
    "\n",
    "  # Creating a table object\n",
    "  table = doc.add_table(rows=1, cols=6)\n",
    "  table.style = 'Table Grid'\n",
    "  # Adding heading in the 1st row of the table\n",
    "  row = table.rows[0].cells\n",
    "  row[0].text = 'Reported Value'\n",
    "  row[1].text = 'Multiplier'\n",
    "  row[2].text = 'Currency'\n",
    "  row[3].text = 'Granular Concept'\n",
    "  row[4].text = 'Umbrella Concept'\n",
    "  row[5].text = 'Sentence'\n",
    "\n",
    "  # Adding data from the list to the table\n",
    "  for item in list_presentation:\n",
    "\n",
    "    #Skipping the Operator as we don't need any of thosea info\n",
    "    if item['speaker'] == \"Operator\":\n",
    "      continue\n",
    "\n",
    "    labels = item['entity']\n",
    "    #print(labels.text)\n",
    "\n",
    "    row_a = table.add_row()\n",
    "    row=row_a.cells\n",
    "    row[5].text = labels.text\n",
    "\n",
    "    for ent in labels.ents:\n",
    "      if ent.label_ == \"REPORTED VALUE\":\n",
    "        row[0].text = row[0].text +\"\\r\"+ent.text\n",
    "      if ent.label_ == \"MULTIPLIER\":\n",
    "        row[1].text = row[1].text +\"\\r\"+ent.text\n",
    "      if ent.label_ == \"CURRENCY\":\n",
    "        row[2].text = row[2].text +\"\\r\"+ent.text\n",
    "      if ent.label_ == \"GRANULAR CONCEPT\":\n",
    "        row[3].text = row[3].text +\"\\r\"+ent.text\n",
    "        row[4].text = \"\\r\"+detect_umbrella_concept(ent.text)\n",
    "\n",
    "      #print(len(labels.ents))\n",
    "      #print(ent.text + \"........\" + ent.label_)\n",
    "\n",
    "\n",
    "def infer_data():\n",
    "  list_presentation = list()\n",
    "  list_qanda = list()\n",
    "  current_list = list_presentation\n",
    "\n",
    "  for item in parags:\n",
    "    key = item['speaker']\n",
    "    t = ''.join(item['paragraph'])\n",
    "    spt = t.replace(';', '$split$').replace('. ', '$split$').split('$split$')\n",
    "    #print(key+\"----------------\")\n",
    "    for line in spt:\n",
    "\n",
    "      if \"QUESTIONS AND ANSWERS\" in line:\n",
    "        current_list = list_qanda\n",
    "        continue\n",
    "\n",
    "      doc = nlp(line)\n",
    "      if len(doc.ents):\n",
    "        current_list.append({'speaker':key,'entity':doc})\n",
    "\n",
    "  return list_presentation, list_qanda\n",
    "\n",
    "def add_bullets(doc,list_item):\n",
    "  list_sorted = sorted(list_item, key=lambda d: d['speaker'])\n",
    "  previous_speaker = \"\"\n",
    "  for item in list_sorted:\n",
    "    sentence = item['entity']\n",
    "    current_speaker = item['speaker']\n",
    "    if current_speaker is not previous_speaker:\n",
    "      # Adding list of style name 'List Bullet'\n",
    "      doc.add_heading(item['speaker'], 3)\n",
    "\n",
    "    # Adding points to the list named 'List Number'\n",
    "    doc.add_paragraph(sentence.text,style='List Bullet')\n",
    "\n",
    "    previous_speaker = item['speaker']\n",
    "\n",
    "def add_heading(title):\n",
    "    # Add another paragraph\n",
    "    doc.add_paragraph()\n",
    "    p = doc.add_paragraph()\n",
    "    #  Add a run and format it\n",
    "    run = p.add_run(title)\n",
    "    run.bold = True\n",
    "    run.font.name = 'Arial'\n",
    "    run.font.size = docx.shared.Pt(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer presentation sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "rz5VXMy4NHsh"
   },
   "outputs": [],
   "source": [
    "list_presentation, list_qanda = infer_data()\n",
    "list_presentation_cleaned = clean_data_from_infer_errors(list_presentation)\n",
    "list_qanda_cleaned = clean_data_from_infer_errors(list_qanda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create summarization and document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "nTXR7ggeRH7e"
   },
   "outputs": [],
   "source": [
    "# Create a document\n",
    "doc = docx.Document()\n",
    "\n",
    "add_title(doc)\n",
    "add_corp_partecipants(doc)\n",
    "add_conf_partecipants(doc)\n",
    "add_fin_table(doc, list_presentation_cleaned)\n",
    "add_heading(\"PRESENTATION\")\n",
    "add_bullets(doc,list_presentation_cleaned)\n",
    "add_heading(\"QUESTIONS AND ANSWERS\")\n",
    "add_bullets(doc,list_qanda_cleaned)\n",
    "\n",
    "# Save the document\n",
    "doc.save(\"Summarization - \"+title+\".docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This application extract partecipants and paragraphs from the transcript , and will calculate the sentences from each paragraph splitting on . and ; for each call partecipant, exclueded the Operator.\n",
    "Each sentence will be infered into the model, and if it recognizes any entity (<b>Reported Value, Multiplier, Currency and Granular Concept</b>) with a valid numeric Reported Value, then it will be added into the finacial table as well as into the bullet points list for whom said that sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caveats\n",
    "The only caveats is that i haven't found a good way to calculate the Umbrella Concept, as it's not present in the transcript explicitely.\n",
    "The ways it works now is with a dictionary that map the known <b>Granular Concept</b> with the known <b>Umbrella Concept</b>.\n",
    "One idea that i had but didn't have time to implement is to create another model (classification) that is trained to classify Granular concepts, and the output will be the Umbrella Concept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOd0dQw1PtVtZj6v++wkM9R",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
